{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 569)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "# Generate random permutation of the dataset\n",
    "idx = np.random.permutation(X.shape[0])\n",
    "X, Y = X[idx], Y[idx]\n",
    "\n",
    "#Normalize the dataset\n",
    "X = X.T\n",
    "X = (X-np.mean(X, axis=1, keepdims = True))/(np.max(X, axis=1, keepdims = True)-np.min(X, axis = 1, keepdims = True))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X(Features) is: (30, 569)\n",
      "The shape of Y(Target values) is: (1, 569)\n",
      "Number of training examples: 569\n"
     ]
    }
   ],
   "source": [
    "shape_X = X.shape\n",
    "Y = Y.reshape(1,Y.shape[0])\n",
    "shape_Y = Y.shape\n",
    "m = X.shape[1]  # training set size\n",
    "\n",
    "# Type: numpy array, validate using function type(X), type(Y)\n",
    "print ('The shape of X(Features) is: ' + str(shape_X))\n",
    "print ('The shape of Y(Target values) is: ' + str(shape_Y))\n",
    "print ('Number of training examples:', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part (W.X + b) of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Activation Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-Layer Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 #number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost =  (-1./m) * np.sum(np.multiply(np.log(AL),Y) + np.multiply(np.log(1-AL),1-Y))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1./m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1./m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Activation Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-Layer Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients.\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.05, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (features, number of examples)\n",
    "    Y -- vector of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i+100, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: 0.693157\n",
      "Cost after iteration 200: 0.663114\n",
      "Cost after iteration 300: 0.660511\n",
      "Cost after iteration 400: 0.660242\n",
      "Cost after iteration 500: 0.660173\n",
      "Cost after iteration 600: 0.660103\n",
      "Cost after iteration 700: 0.659998\n",
      "Cost after iteration 800: 0.659828\n",
      "Cost after iteration 900: 0.659536\n",
      "Cost after iteration 1000: 0.658991\n",
      "Cost after iteration 1100: 0.657856\n",
      "Cost after iteration 1200: 0.655079\n",
      "Cost after iteration 1300: 0.646439\n",
      "Cost after iteration 1400: 0.607756\n",
      "Cost after iteration 1500: 0.431252\n",
      "Cost after iteration 1600: 0.253743\n",
      "Cost after iteration 1700: 0.171894\n",
      "Cost after iteration 1800: 0.131162\n",
      "Cost after iteration 1900: 0.108596\n",
      "Cost after iteration 2000: 0.095019\n",
      "Cost after iteration 2100: 0.086291\n",
      "Cost after iteration 2200: 0.080382\n",
      "Cost after iteration 2300: 0.076191\n",
      "Cost after iteration 2400: 0.073076\n",
      "Cost after iteration 2500: 0.070669\n",
      "Cost after iteration 2600: 0.068732\n",
      "Cost after iteration 2700: 0.067119\n",
      "Cost after iteration 2800: 0.065738\n",
      "Cost after iteration 2900: 0.064528\n",
      "Cost after iteration 3000: 0.063447\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dimensions of each layer in our network\n",
    "layer_dims = [X.shape[0],20,10,Y.shape[0]]\n",
    "parameters = L_layer_model(X, Y, layer_dims, num_iterations = 3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- data, numpy array of shape (features, number of examples)    \n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    AL, caches = L_model_forward(X,parameters)\n",
    "    predictions = 1 * (AL > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97%\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
