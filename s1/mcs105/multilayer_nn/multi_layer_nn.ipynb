{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 569)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "# Generate random permutation of the dataset\n",
    "idx = np.random.permutation(X.shape[0])\n",
    "X, Y = X[idx], Y[idx]\n",
    "\n",
    "#Normalize the dataset\n",
    "X = X.T\n",
    "X = (X-np.mean(X, axis=1, keepdims = True))/(np.max(X, axis=1, keepdims = True)-np.min(X, axis = 1, keepdims = True))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X(Features) is: (30, 569)\n",
      "The shape of Y(Target values) is: (1, 569)\n",
      "Number of training examples: 569\n"
     ]
    }
   ],
   "source": [
    "shape_X = X.shape\n",
    "Y = Y.reshape(1,Y.shape[0])\n",
    "shape_Y = Y.shape\n",
    "m = X.shape[1]  # training set size\n",
    "\n",
    "# Type: numpy array, validate using function type(X), type(Y)\n",
    "print ('The shape of X(Features) is: ' + str(shape_X))\n",
    "print ('The shape of Y(Target values) is: ' + str(shape_Y))\n",
    "print ('Number of training examples:', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part (W.X + b) of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Activation Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-Layer Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 #number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost =  (-1./m) * np.sum(np.multiply(np.log(AL),Y) + np.multiply(np.log(1-AL),1-Y))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1./m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1./m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Activation Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-Layer Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients.\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.1, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (features, number of examples)\n",
    "    Y -- vector of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693148\n",
      "Cost after iteration 100: 0.660567\n",
      "Cost after iteration 200: 0.660314\n",
      "Cost after iteration 300: 0.660309\n",
      "Cost after iteration 400: 0.660306\n",
      "Cost after iteration 500: 0.660301\n",
      "Cost after iteration 600: 0.660295\n",
      "Cost after iteration 700: 0.660285\n",
      "Cost after iteration 800: 0.660271\n",
      "Cost after iteration 900: 0.660249\n",
      "Cost after iteration 1000: 0.660209\n",
      "Cost after iteration 1100: 0.660135\n",
      "Cost after iteration 1200: 0.659979\n",
      "Cost after iteration 1300: 0.659592\n",
      "Cost after iteration 1400: 0.658355\n",
      "Cost after iteration 1500: 0.652160\n",
      "Cost after iteration 1600: 0.575392\n",
      "Cost after iteration 1700: 0.179987\n",
      "Cost after iteration 1800: 0.104007\n",
      "Cost after iteration 1900: 0.083656\n",
      "Cost after iteration 2000: 0.075042\n",
      "Cost after iteration 2100: 0.070308\n",
      "Cost after iteration 2200: 0.067151\n",
      "Cost after iteration 2300: 0.064748\n",
      "Cost after iteration 2400: 0.062755\n",
      "Cost after iteration 2500: 0.061021\n",
      "Cost after iteration 2600: 0.059467\n",
      "Cost after iteration 2700: 0.058052\n",
      "Cost after iteration 2800: 0.056750\n",
      "Cost after iteration 2900: 0.055547\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dimensions of each layer in our network\n",
    "layer_dims = [X.shape[0],20,20,Y.shape[0]]\n",
    "parameters = L_layer_model(X, Y, layer_dims, num_iterations = 3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- data, numpy array of shape (features, number of examples)    \n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    AL, caches = L_model_forward(X,parameters)\n",
    "    predictions = 1 * (AL > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98%\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
