{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of multi_layer_nn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "SVSu7fWsnxve",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Things to do!\n",
        "\n",
        "+ Add a layer activation argument to L_layer_model fucntion and propagate that feature to functions in it. This activation argument is a list of size of number of layers. Eg. [\"relu\", \"relu\", \"'sigmoid\"].\n",
        "\n",
        "+ Train and Test model with different activation functions.\n",
        "+ ok\n"
      ]
    },
    {
      "metadata": {
        "id": "fKKcm2WxTL_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Package imports"
      ]
    },
    {
      "metadata": {
        "id": "mJDR_lRUTL_Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yy0UZCnOTL_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset Init"
      ]
    },
    {
      "metadata": {
        "id": "g0CZAGWHTL_s",
        "colab_type": "code",
        "outputId": "21c6884b-5ae0-40e8-80ab-6380aac3208c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dataset = load_breast_cancer()\n",
        "X = dataset.data\n",
        "Y = dataset.target\n",
        "\n",
        "# Generate random permutation of the dataset\n",
        "idx = np.random.permutation(X.shape[0])\n",
        "X, Y = X[idx], Y[idx]\n",
        "\n",
        "#Normalize the dataset\n",
        "X = X.T\n",
        "X = (X-np.mean(X, axis=1, keepdims = True))/(np.max(X, axis=1, keepdims = True)-np.min(X, axis = 1, keepdims = True))\n",
        "print(X.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30, 569)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HQhKShdzTL_-",
        "colab_type": "code",
        "outputId": "a7b387ed-66c4-4861-8726-fb87c1a7609a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "shape_X = X.shape\n",
        "Y = Y.reshape(1,Y.shape[0])\n",
        "shape_Y = Y.shape\n",
        "m = X.shape[1]  # training set size\n",
        "\n",
        "# Type: numpy array, validate using function type(X), type(Y)\n",
        "print ('The shape of X(Features) is: ' + str(shape_X))\n",
        "print ('The shape of Y(Target values) is: ' + str(shape_Y))\n",
        "print ('Number of training examples:', m)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of X(Features) is: (30, 569)\n",
            "The shape of Y(Target values) is: (1, 569)\n",
            "Number of training examples: 569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wl3e_2ekTMAN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2-layer Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "EEcTwjh6TMAQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Developing Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "FHJyLybCTMAT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ]
    },
    {
      "metadata": {
        "id": "BWqghNLnTMAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            \n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGJBB6uRTMAj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Forward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "Je5VmJ8dTMAq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Activation Functions"
      ]
    },
    {
      "metadata": {
        "id": "kn5JqNC0TMA1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "def tanh(Z):\n",
        "    \"\"\"\n",
        "    Implements the tanh activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of tanh(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    \n",
        "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "  \n",
        "def relu_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "  \n",
        "def tanh_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single TANH unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "\n",
        "    s = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
        "    dZ = dA * (1 - s * s)\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "\n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6k9YU5ETMBC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Linear Forward"
      ]
    },
    {
      "metadata": {
        "id": "V7pHXO3ITMBH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part (W.X + b) of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = np.dot(W, A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9HPXjauSTMBk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Linear Activation Forward"
      ]
    },
    {
      "metadata": {
        "id": "M_ib9ADLTMBm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "        \n",
        "    elif activation == \"tanh\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = tanh(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ecEnqj7STMBu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### L-Layer Forward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "BQrLWTbQTMBw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def L_model_forward(X, parameters, activation_functions):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
        "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2 #number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = activation_functions[l-1])\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation = activation_functions[L-1])\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yw0CZef0TMB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Cost Function"
      ]
    },
    {
      "metadata": {
        "id": "jFlZTLeRTMB8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost =  (-1./m) * np.sum(np.multiply(np.log(AL),Y) + np.multiply(np.log(1-AL),1-Y))\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nuBFb_KGTMCK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Backward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "pty_f7CbTMCO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Linear Backward"
      ]
    },
    {
      "metadata": {
        "id": "yxZZSIzkTMCT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1./m)*np.dot(dZ, A_prev.T)\n",
        "    db = (1./m)*np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jY6PJRefTMCb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Linear Activation Backward"
      ]
    },
    {
      "metadata": {
        "id": "GXXPSohBTMCd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"tanh\":\n",
        "        dZ = tanh_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQMaTnEpTMCm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### L-Layer Backward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "_ZCm4nSWTMCs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches, activation_functions):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients.\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = activation_functions[L-1])\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = activation_functions[l])\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRu2m5B6TMC2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Update Parameters"
      ]
    },
    {
      "metadata": {
        "id": "MDXEUkirTMC7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qrlHmbttTMDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Network Model"
      ]
    },
    {
      "metadata": {
        "id": "xDhaDxyrTMDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def L_layer_model(X, Y, layers_dims, activation_functions, learning_rate = 0.1, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (features, number of examples)\n",
        "    Y -- vector of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization.\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    activation_function = \"relu\"\n",
        "    #activation_functions.insert(0, activation_function)\n",
        "    # gradient descent\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        #activation function for first L-1 layers, last layer would always have sigmoid\n",
        "        AL, caches = L_model_forward(X, parameters,activation_functions)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, Y, caches, activation_functions)\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i+100, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "siiYK9pLTMDY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "3rFv4v0kTMDa",
        "colab_type": "code",
        "outputId": "00989b67-275c-4bc1-8659-bdc5e2d531dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        }
      },
      "cell_type": "code",
      "source": [
        "#Dimensions of each layer in our network\n",
        "layer_dims = [X.shape[0],20,10,Y.shape[0]]\n",
        "activation_functions = [\"tanh\",\"tanh\",\"sigmoid\"]\n",
        "alpha = 0.1\n",
        "parameters = L_layer_model(X, Y, layer_dims, activation_functions, learning_rate = alpha, num_iterations = 3000, print_cost=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 100: 0.693148\n",
            "Cost after iteration 200: 0.660566\n",
            "Cost after iteration 300: 0.660312\n",
            "Cost after iteration 400: 0.660304\n",
            "Cost after iteration 500: 0.660297\n",
            "Cost after iteration 600: 0.660285\n",
            "Cost after iteration 700: 0.660267\n",
            "Cost after iteration 800: 0.660236\n",
            "Cost after iteration 900: 0.660175\n",
            "Cost after iteration 1000: 0.660039\n",
            "Cost after iteration 1100: 0.659671\n",
            "Cost after iteration 1200: 0.658255\n",
            "Cost after iteration 1300: 0.646988\n",
            "Cost after iteration 1400: 0.362383\n",
            "Cost after iteration 1500: 0.134166\n",
            "Cost after iteration 1600: 0.095446\n",
            "Cost after iteration 1700: 0.081343\n",
            "Cost after iteration 1800: 0.074547\n",
            "Cost after iteration 1900: 0.070521\n",
            "Cost after iteration 2000: 0.067708\n",
            "Cost after iteration 2100: 0.065506\n",
            "Cost after iteration 2200: 0.063646\n",
            "Cost after iteration 2300: 0.061999\n",
            "Cost after iteration 2400: 0.060496\n",
            "Cost after iteration 2500: 0.059099\n",
            "Cost after iteration 2600: 0.057793\n",
            "Cost after iteration 2700: 0.056573\n",
            "Cost after iteration 2800: 0.055439\n",
            "Cost after iteration 2900: 0.054393\n",
            "Cost after iteration 3000: 0.053436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fc744d86ef0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zw2gp34HTMDt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predictions and Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "_yDqQez6TMD0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(parameters, X, activation_functions):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- data, numpy array of shape (features, number of examples)    \n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    AL, caches = L_model_forward(X,parameters,activation_functions)\n",
        "    predictions = 1 * (AL > 0.5)\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5-V14UQhTMD6",
        "colab_type": "code",
        "outputId": "24af213c-238d-42e9-c39c-6015659ef06c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "activations = [\"relu\",\"relu\",\"sigmoid\"]\n",
        "predictions = predict(parameters, X, activations)\n",
        "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 98%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8tD8e-DsTwXr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9cd45c0-1d0b-4156-bb05-04bd0ba5a587"
      },
      "cell_type": "code",
      "source": [
        "activations = [\"relu\",\"relu\",\"relu\"]\n",
        "predictions = predict(parameters, X, activations)\n",
        "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SYTxT7vH2emz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f7855b0-afda-4f3c-a7d7-fdd24aa3910c"
      },
      "cell_type": "code",
      "source": [
        "activations = [\"tanh\",\"tanh\",\"sigmoid\"]\n",
        "predictions = predict(parameters, X, activations)\n",
        "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 98%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hBF07oao2lIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a7907dc-66a1-42af-82e3-43d19837469b"
      },
      "cell_type": "code",
      "source": [
        "activations = [\"relu\",\"tanh\",\"sigmoid\"]\n",
        "predictions = predict(parameters, X, activations)\n",
        "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 98%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "89Tje0u747-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}